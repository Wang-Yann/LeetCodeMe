#!/usr/bin/env python
# -*- coding: utf-8 -*-
# @Author        : Rock Wayne 
# @Created       : 2020-08-09 17:17:17
# @Last Modified : 2020-08-09 17:17:17
# @Mail          : lostlorder@gmail.com
# @Version       : alpha-1.0

"""
# ç»™ä½ ä¸€ä¸ªåˆå§‹åœ°å€ startUrl å’Œä¸€ä¸ª HTML è§£æå™¨æ¥å£ HtmlParserï¼Œè¯·ä½ å®ç°ä¸€ä¸ª å¤šçº¿ç¨‹çš„ç½‘é¡µçˆ¬è™«ï¼Œç”¨äºè·å–ä¸ startUrl æœ‰ ç›¸
# åŒä¸»æœºå çš„æ‰€æœ‰é“¾æ¥ã€‚ 
# 
#  ä»¥ ä»»æ„ é¡ºåºè¿”å›çˆ¬è™«è·å–çš„è·¯å¾„ã€‚ 
# 
#  çˆ¬è™«åº”è¯¥éµå¾ªï¼š 
# 
#  
#  ä» startUrl å¼€å§‹ 
#  è°ƒç”¨ HtmlParser.getUrls(url) ä»æŒ‡å®šç½‘é¡µè·¯å¾„è·å¾—çš„æ‰€æœ‰è·¯å¾„ã€‚ 
#  ä¸è¦æŠ“å–ç›¸åŒçš„é“¾æ¥ä¸¤æ¬¡ã€‚ 
#  ä»…æµè§ˆä¸ startUrl ç›¸åŒä¸»æœºå çš„é“¾æ¥ã€‚ 
#  
# 
#  
# 
#  å¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œä¸»æœºåæ˜¯ example.org ã€‚ç®€å•èµ·è§ï¼Œä½ å¯ä»¥å‡è®¾æ‰€æœ‰é“¾æ¥éƒ½é‡‡ç”¨ http åè®®ï¼Œå¹¶ä¸”æ²¡æœ‰æŒ‡å®š ç«¯å£å·ã€‚ä¸¾ä¸ªä¾‹å­ï¼Œé“¾æ¥ http://l
# eetcode.com/problems å’Œé“¾æ¥ http://leetcode.com/contest å±äºåŒä¸€ä¸ª ä¸»æœºåï¼Œ è€Œ http://example
# .org/test ä¸ http://example.com/abc å¹¶ä¸å±äºåŒä¸€ä¸ª ä¸»æœºåã€‚ 
# 
#  HtmlParser çš„æ¥å£å®šä¹‰å¦‚ä¸‹ï¼š 
# 
#  
# interface HtmlParser {
#   // Return a list of all urls from a webpage of given url.
#   // This is a blocking call, that means it will do HTTP request and return wh
# en this request is finished.
#   public List<String> getUrls(String url);
# } 
# 
#  æ³¨æ„ä¸€ç‚¹ï¼ŒgetUrls(String url) æ¨¡æ‹Ÿæ‰§è¡Œä¸€ä¸ªHTTPçš„è¯·æ±‚ã€‚ ä½ å¯ä»¥å°†å®ƒå½“åšä¸€ä¸ªé˜»å¡å¼çš„æ–¹æ³•ï¼Œç›´åˆ°è¯·æ±‚ç»“æŸã€‚ getUrls(Strin
# g url) ä¿è¯ä¼šåœ¨ 15ms å†…è¿”å›æ‰€æœ‰çš„è·¯å¾„ã€‚ å•çº¿ç¨‹çš„æ–¹æ¡ˆä¼šè¶…è¿‡æ—¶é—´é™åˆ¶ï¼Œä½ èƒ½ç”¨å¤šçº¿ç¨‹æ–¹æ¡ˆåšçš„æ›´å¥½å—ï¼Ÿ 
# 
#  å¯¹äºé—®é¢˜æ‰€éœ€çš„åŠŸèƒ½ï¼Œä¸‹é¢æä¾›äº†ä¸¤ä¸ªä¾‹å­ã€‚ä¸ºäº†æ–¹ä¾¿è‡ªå®šä¹‰æµ‹è¯•ï¼Œä½ å¯ä»¥å£°æ˜ä¸‰ä¸ªå˜é‡ urlsï¼Œedges å’Œ startUrlã€‚ä½†è¦æ³¨æ„ä½ åªèƒ½åœ¨ä»£ç ä¸­è®¿é—® s
# tartUrlï¼Œå¹¶ä¸èƒ½ç›´æ¥è®¿é—® urls å’Œ edgesã€‚ 
# 
#  
# 
#  æ‹“å±•é—®é¢˜ï¼š 
# 
#  
#  å‡è®¾æˆ‘ä»¬è¦è¦æŠ“å– 10000 ä¸ªèŠ‚ç‚¹å’Œ 10 äº¿ä¸ªè·¯å¾„ã€‚å¹¶ä¸”åœ¨æ¯ä¸ªèŠ‚ç‚¹éƒ¨ç½²ç›¸åŒçš„çš„è½¯ä»¶ã€‚è½¯ä»¶å¯ä»¥å‘ç°æ‰€æœ‰çš„èŠ‚ç‚¹ã€‚æˆ‘ä»¬å¿…é¡»å°½å¯èƒ½å‡å°‘æœºå™¨ä¹‹é—´çš„é€šè®¯ï¼Œå¹¶ç¡®ä¿æ¯
# ä¸ªèŠ‚ç‚¹è´Ÿè½½å‡è¡¡ã€‚ä½ å°†å¦‚ä½•è®¾è®¡è¿™ä¸ªç½‘é¡µçˆ¬è™«ï¼Ÿ 
#  å¦‚æœæœ‰ä¸€ä¸ªèŠ‚ç‚¹å‘ç”Ÿæ•…éšœä¸å·¥ä½œè¯¥æ€ä¹ˆåŠï¼Ÿ 
#  å¦‚ä½•ç¡®è®¤çˆ¬è™«ä»»åŠ¡å·²ç»å®Œæˆï¼Ÿ 
#  
# 
#  
# 
#  ç¤ºä¾‹ 1ï¼š 
# 
#  
# 
#  
# è¾“å…¥ï¼š
# urls = [
# Â  "http://news.yahoo.com",
# Â  "http://news.yahoo.com/news",
# Â  "http://news.yahoo.com/news/topics/",
# Â  "http://news.google.com",
# Â  "http://news.yahoo.com/us"
# ]
# edges = [[2,0],[2,1],[3,2],[3,1],[0,4]]
# startUrl = "http://news.yahoo.com/news/topics/"
# è¾“å‡ºï¼š[
# Â  "http://news.yahoo.com",
# Â  "http://news.yahoo.com/news",
# Â  "http://news.yahoo.com/news/topics/",
# Â  "http://news.yahoo.com/us"
# ]
#  
# 
#  ç¤ºä¾‹ 2ï¼š 
# 
#  
# 
#  
# è¾“å…¥ï¼š
# urls = [
# Â  "http://news.yahoo.com",
# Â  "http://news.yahoo.com/news",
# Â  "http://news.yahoo.com/news/topics/",
# Â  "http://news.google.com"
# ]
# edges = [[0,2],[2,1],[3,2],[3,1],[3,0]]
# startUrl = "http://news.google.com"
# è¾“å‡ºï¼š["http://news.google.com"]
# è§£é‡Šï¼šstartUrl é“¾æ¥ä¸å…¶ä»–é¡µé¢ä¸å…±äº«ä¸€ä¸ªä¸»æœºåã€‚ 
# 
#  
# 
#  æç¤ºï¼š 
# 
#  
#  1 <= urls.length <= 1000 
#  1 <= urls[i].length <= 300 
#  startUrl æ˜¯ urls ä¸­çš„ä¸€ä¸ªã€‚ 
#  ä¸»æœºåçš„é•¿åº¦å¿…é¡»ä¸º 1 åˆ° 63 ä¸ªå­—ç¬¦ï¼ˆåŒ…æ‹¬ç‚¹ . åœ¨å†…ï¼‰ï¼Œåªèƒ½åŒ…å«ä» â€œaâ€ åˆ° â€œzâ€ çš„ ASCII å­—æ¯å’Œ â€œ0â€ åˆ° â€œ9â€ çš„æ•°å­—ï¼Œä»¥åŠä¸­åˆ’
# çº¿ â€œ-â€ã€‚ 
#  ä¸»æœºåå¼€å¤´å’Œç»“å°¾ä¸èƒ½æ˜¯ä¸­åˆ’çº¿ â€œ-â€ã€‚ 
#  å‚è€ƒèµ„æ–™ï¼šhttps://en.wikipedia.org/wiki/Hostname#Restrictions_on_valid_hostnames 
#  ä½ å¯ä»¥å‡è®¾è·¯å¾„éƒ½æ˜¯ä¸é‡å¤çš„ã€‚ 
#  
#  Related Topics æ·±åº¦ä¼˜å…ˆæœç´¢ å¹¿åº¦ä¼˜å…ˆæœç´¢ 
#  ğŸ‘ 12 ğŸ‘ 0
	 

"""

import collections
from typing import List

import pytest


class HtmlParser(object):

    def getUrls(self, url):

        idx = urls.index(url)
        if idx == -1:
            return []
        neighbors = [t for f, t in edges if f == idx]
        return [urls[i] for i in neighbors]


# leetcode submit region begin(Prohibit modification and deletion)
# """
# This is HtmlParser's API interface.
# You should not implement it, or speculate about its implementation
# """
# class HtmlParser(object):
#    def getUrls(self, url):
#        """
#        :type url: str
#        :rtype List[str]
#        """
import threading
from urllib import parse
import queue


class Solution:
    NUMBER_OF_WORKERS = 8

    def __init__(self):
        self.__cv = threading.Condition()
        self.__q = collections.deque()
        self.__working_count = 0

    def crawl(self, startUrl: str, htmlParser: 'HtmlParser') -> List[str]:
        def hostname(url):
            domain_parts = parse.urlparse(url)
            return "{}://{}".format(domain_parts.scheme, domain_parts.netloc)

        def worker(htmlParser, lookup):
            while True:
                from_url = self.__q.get()
                if from_url is None:
                    break
                name = hostname(from_url)
                for to_url in htmlParser.getUrls(from_url):
                    if name != hostname(to_url):
                        continue
                    with self.__cv:
                        if to_url not in lookup:
                            lookup.add(to_url)
                            self.__q.put(to_url)
                self.__q.task_done()

        workers = []
        self.__q = queue.Queue()
        self.__q.put(startUrl)
        lookup = {startUrl}
        for i in range(self.NUMBER_OF_WORKERS):
            t = threading.Thread(target=worker, args=(htmlParser, lookup))
            t.start()
            workers.append(t)
        self.__q.join()
        for t in workers:
            self.__q.put(None)
        for t in workers:
            t.join()
        return list(lookup)


# leetcode submit region end(Prohibit modification and deletion)


@pytest.mark.parametrize("kw,expected", [
    [dict(
        urls=[
            "http://news.yahoo.com",
            "http://news.yahoo.com/news",
            "http://news.yahoo.com/news/topics/",
            "http://news.google.com",
            "http://news.yahoo.com/us"
        ],
        edges=[[2, 0], [2, 1], [3, 2], [3, 1], [0, 4]],
        startUrl="http://news.yahoo.com/news/topics/"

    ),
        [
            "http://news.yahoo.com",
            "http://news.yahoo.com/news",
            "http://news.yahoo.com/news/topics/",
            "http://news.yahoo.com/us"
        ]
    ],
    [dict(
        urls=[
            "http://news.yahoo.com",
            "http://news.yahoo.com/news",
            "http://news.yahoo.com/news/topics/",
            "http://news.google.com"
        ],
        edges=[[0, 2], [2, 1], [3, 2], [3, 1], [3, 0]],
        startUrl="http://news.google.com"

    ),
        ["http://news.google.com"]
    ],

])
def test_solutions(kw, expected):
    global urls, edges
    urls = kw.pop("urls")
    edges = kw.pop("edges")
    kw["htmlParser"] = HtmlParser()
    ans = Solution().crawl(**kw)
    print(ans)
    assert ans == expected


if __name__ == '__main__':
    pytest.main(["-q", "--color=yes", "--capture=tee-sys", __file__])
